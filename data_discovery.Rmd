---
title: "Data Discovery"
output: html_notebook
---

```{r}
library("dplyr")
library("magrittr")
library("ggplot2")
library("fiftystater")
```

## Data Discovery

We will first get a feel for our data set by getting a summary of the dataframe `churn_df`.

```{r}
churn_df <- read.csv("data/churn_train.csv", na.strings = c("", "NA"))

summary(churn_df)
```

From the summary we can see that a lot of `NA` values are present in many columns execpt for the columns `state`, `aread_code`, `international_plan`, `voice_mail_plan`, `total_night_calls`, and `churn`.

```{r}
churn_df %>%
  select_if(is.factor) %>%
  names()
```

`state`, `aread_code`, `international_plan`, `voice_mail_plan`, and `churn` are columns that are of type factor.

The `state` columns seems to represent states from the USA (51 states in total). Below is a frequency table showing us the where most of the customers come from.

```{r}
state_freq <- churn_df %>%
  select(state) %>%
  group_by(state) %>%
  summarise(freq  = n()) %>%
  arrange(desc(freq))

summary(state_freq)
```

```{r}
us_map_data <- merge(
  fifty_states,
  data.frame(
    # Add missing state
    state = c(state.abb, "DC"),
    id = tolower(c(state.name, "district of columbia"))
  ),
  by = "id",
  all = T
)

summary(us_map_data)
```

```{r}
state_freq <- merge(state_freq, us_map_data, by = "state") %>%
  select(id, freq, long, lat, group) %>%
  distinct()
```

```{r}
# Using the state.center list we can find out the exact center of each state
state_ctrs <- data.frame(
    state = c(state.abb, "DC"),
    c_long = c(state.center$x, 38.889931) , 
    c_lat = c(state.center$y, -77.009003)
  ) %>% 
  merge(us_map_data, ., by = "state") %>%
  select(state, id, c_long, c_lat) %>%
  distinct()

View(state_ctrs)
```
  
```{r}
ggplot(us_map_data, aes(x = long, y = lat)) +
  geom_map(
    map = us_map_data,
    color="#ffffff",
    aes(map_id = id)
  ) +
  geom_map(
    data = state_freq,
    map = us_map_data,
     color="#ffffff",
    aes(fill = freq, map_id = id)
  ) +
  scale_fill_continuous(low = 'thistle2', high = 'darkred', guide='colorbar') +
  geom_text(
    data = state_ctrs,
    size = 2,
    aes(x = c_long, y = c_lat, label = state)
  ) +
  coord_cartesian(xlim = c(-130, -65), ylim = c(24, 51)) +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank(),
    panel.background = element_blank(),
    panel.border = element_blank()
  ) +
  labs(x = NULL, y = NULL, title = "Customers per State")
```


16 out of the 20 variables (columns) have `NA` values. Further analysis of the summary of our dataframe reveals that 10 variables have about __200__ `NA` values while 2 have __301__ and 1 has __501__.

Fu

For a better understanding of the presence of `NA` in our dataframe let's look at the percentage of `NA`s accross all the variables in the dataframe.

```{r}
# Define a function to compute the percentage of NAs accross all columns.
na_percentage <- function(df, fmt = F) {
  return (df %>%
            is.na() %>%
            colMeans() %>%
            sapply(function(x) {
              if (fmt) {
                return(sprintf("%.5f%%", x * 100))
              }
              
              return (x)
            })
          )
}

na_df <- na_percentage(churn_df) %>%
  data_frame(Columns = names(.), `NA %` = .) %>%
  mutate_at(
    vars(`NA %`),
    funs(round(. * 100, 2))
  ) %>%
  mutate(label = sprintf("%g%%", `NA %`)) %>%
  arrange(desc(`NA %`))

na_df %>% select(-label)
```

The table above lists all the variables (columns) and their respective percentage of `NA`s. We can see that most categorical variables such as `state`, `area_code`, `international_plan`, etc. including `total_night_calls` (numerical variable) have no `NA` values in them.

The bar chart below provides a visual representation of the percentage of `NA` in the dataset. We can see that `account_length`, `total_intl_calls` and `total_intl_charge` contribute the most `NA`s with `account_length` being the top contributor.

```{r}
na_df %>%
  filter(`NA %`> 0) %>%
  ggplot(aes(x = Columns, y = `NA %`, fill = Columns)) +
  geom_bar(stat="identity") +
  guides(fill = F) +
  coord_flip() +
  geom_text(aes(label = label), hjust = 1.6, size = 3.5) +
  theme_minimal()
```

```{r, echo = F}
# This code chunk will not be part of the knitted document.

# Show the index number of each column names
churn_df %>%
  colnames() %>%
  data.frame(index = c(1:length(.)), colname = .)
```


Let's look at the columns that have __6.0060%__ population of `NA` values and  try to find a pattern in the way `NA` values are distributed.

```{r}
churn_df %>%
  select(2, 6:13, 15:17, 18, 19) %>%
  filter_all(any_vars(is.na(.))) 
```

We can see that there are many `NA` values present in enitre row which makes it .....

301 rows were `NA` exists blah blah blah....

We then should remove rows that have more than 75%  of it are `NA`

```{r}
churn_df_1 <- churn_df[rowMeans(is.na(churn_df)) <= 0.25,]
summary(churn_df_1)
```


Lets look at the percentage of `NA` and how it has changed
```{r}
na_df_1 <- na_percentage(churn_df_1) %>%
  data_frame(Columns = names(.), `NA %` = .) %>%
  mutate_at(
    vars(`NA %`),
    funs(round(. * 100, 2))
  ) %>%
  mutate(label = sprintf("%g%%", `NA %`)) %>%
  arrange(desc(`NA %`))

na_df_1
```
Let's look at a visual presentation of how the `NA` values have changed

```{r}
na_df_1 %>%
  filter(`NA %`> 0) %>%
  ggplot(aes(x = Columns, y = `NA %`, fill = Columns)) +
  geom_bar(stat="identity") +
  guides(fill = F) +
  coord_flip() +
  geom_text(aes(label = label), hjust = 1.6, size = 3.5) +
  theme_minimal()
```

```{r}
churn_df_1 %>%
  select(2, 6:13, 15:17, 18, 19) %>%
  filter_all(any_vars(is.na(.)))
```

We have removed about 200 rows where the population of `NA`s were 75% reducing our overall dataset from __3333__ to __3133__.

Now that we have removed rows which had very little information in them, we can focus on figureing out a strategy on filling in the missing values of our remaning rows.

```{r}
sum(is.na(churn_df_1))
```



```{r}

churn_df_acclength <- lapply(churn_df_1[,2], abs)
churn_df_acclength
```

```{r}

index <- na_col_df$account_length < 0
na_col_df$account_length[index] = -abs(na_col_df$account_length[index])

```

```{r}
na_col_df_acclength <- churn_df_acclength %>%
  select(account_length, total_eve_minutes, total_intl_calls, churn) %>%
  transform(churn = ifelse(churn == "no", 0, 1))

na_col_df_acclength
```

//changing all neg values into pos values

```{r}
na_col_df_acclength <- churn_df_acclength %>% na.omit()

summary(na_col_df_acclength)

```



We have __503__ `NA` values in our enitire dataframe meaning that of the columns that have `NA`  each row in the column has at least 1 `NA`.

By looking at the pie charts above we can see that columns `acount_length` has thr most number of `NA`s however we are not convinced that `acount_length` is a significant predictor variable. 

We can build a model which will give us an idea of the variables that are important and that we need to focus on when trying to fill in missing values

```{r}
churn_model <- glm(churn~., data = churn_df_1, family = "binomial")
summary(churn_model)
```

`account_length` had the most number of `NA` however its p-value is high at __0.43339__ which tells us that its not significant when predicting `churn`

```{r}
na_col_df <- churn_df_1 %>%
  select(account_length, total_eve_minutes, total_intl_calls, churn) %>%
  transform(churn = ifelse(churn == "no", 0, 1))

na_col_df
```
we have to remove all rows that have NA in `na_col_df` so as to calculate the correlation between account_length, total_eve_minutes, total_intl_calls and `churn`.

currently in `na_col_df` there are __503__ `NA` values

```{r}
summary(na_col_df)
```


```{r}
na_col_df <- na_col_df %>% na.omit()

summary(na_col_df)
```


Lets do the pearson correlation between `account_length` and `churn`
```{r}
cor(na_col_df)
```

Lets do the spearman correlation between `account_length` and `churn`


```{r}
cor(na_col_df, method = "spearman")
```

Since the correlation between `churn` and `account_length` is __ 0.006978986__ using pearson correlation and __0.0003126797__ using spearman correlation. It means `account_length` is essentially no correlation withchurn so as we transition from 0 to 1 (no to yes) in churn, the account_length is essentially invariant.

Therefore `account_length` is not a predictor of `churn` and we should omit it from our model.


We keep total_eve_minutes and total_intl_calls beacause they are negatively correlated with churn so customers are less apt to churn .....

```{r}
churn_df_2 <- churn_df_1 %>% select(-account_length)

summary(churn_df_2)
```

```{r}
sum(is.na(churn_df_2))
```

Now we have __202__ rows we need to worry about and fill in the `NA` values for.

We will now impute missing values in our dataset.

Now we will save the data frome that has been prepared for data imputation.

```{r}
save(churn_df_2, file = "data/data_exploration.rda")
```

```{r}
churn_train_2 <- replace("account_length")= churn_df("account_length")
```

```{r}
churn_df
```

