---
title: "Data Preparation"
output: html_notebook
---

```{r}
library("dplyr")
library("magrittr")
library("ggplot2")
library("randomForest")
library("DMwR") # for kNN imputation
#library("mice")
```

```{r echo = F}
load("./data/data_exploration.rda")
set.seed(123)
```

```{r}
summary(churn_df_2)
```

## 1. Data Imputation

### 1.1 Data Imputation using Randomforest

For the RandomForest, the proximity matrix is used to update the imputation of the NA's. For continuous predictors, the imputed value is the weighted average of the non-missing observations, where the weights are the proximities. For categorical predictors, the imputed value is the category with the largest average proximity. This process is iterated n times. The below chunk shows the data frame (cdf_rf) after imputing the missing values using the Randomforest method.

```{r results = 'hide'}
cdf_rf.imputed <- rfImpute(churn ~ ., data = churn_df_2)
```


```{r}
summary(cdf_rf.imputed)
```

### 1.2 Data Imputation using kNN

In the below code, we are endeavoring to impute values using the kNN imputation method. 

```{r}
cdf_knn.imputed <- knnImputation(churn_df_2)

summary(cdf_knn.imputed)
```

### 1.3 kNN vs Randomforest for Data Imputation

We are now ready to compare the imputed values obtained using the kNN & RandomForest methods. As seen from the output, approximately 95% percent of the imputed values from either methods, are seen to be identical, hence it can be deduced that imputing missing values is immaterial of the type of imputing method used.

```{r}
rf_knn_df <- data.frame(
  kNN_total_intl_calls = cdf_knn.imputed$total_intl_calls,
  rf_total_intl_calls = cdf_rf.imputed$total_intl_calls,
  kNN_total_eve_minutes = cdf_knn.imputed$total_eve_minutes,
  rf_total_eve_minutes = cdf_rf.imputed$total_eve_minutes
) 

rf_knn_df
```

ggplot is a data visualization package useful for creating complex but elegant plots. The below plots show a visual for the variables total_intl_calls and total_eve_minutes with the imputed values through both the kNN and RandomForest methods.
For total_intl_calls, as the value of x increases, the kNN values are spread across a thin range between 3 & 7 on the y-axis,
For RandomForest, the proximity of the values is high within a range of 0 - 8 on the y-axis. As the vaue of y increases, the values are more widespread and dispersed.


```{r}
ggplot(rf_knn_df) +
  geom_point(aes(
    x = seq_along(kNN_total_intl_calls),
    y = kNN_total_intl_calls, colour = "kNN")
  ) +
  geom_point(aes(
    x = seq_along(rf_total_intl_calls),
    y = rf_total_intl_calls, colour = "RandomForest")
  ) +
  labs(x = "Index", y = "total_intl_calls", title = "kNN vs RandomForest for total_intl_calls")
```

```{r}
ggplot(rf_knn_df) +
  geom_point(aes(
    x = seq_along(kNN_total_eve_minutes),
    y = kNN_total_eve_minutes, colour = "kNN")
  ) +
  geom_point(aes(
    x = seq_along(rf_total_eve_minutes),
    y = rf_total_eve_minutes, colour = "RandomForest")
  ) +
  labs(x = "Index", y = "total_eve_minutes", title = "kNN vs RandomForest for total_eve_minutes")
```

## 2. Stepwise Regression

Stepwise regression is a semi-automated process of building a model by successively adding or removing variables based on the t-statistics of their estimated coefficients. The stepwise option lets you either begin with no variables in the model and proceed forward (i.e. adding one variable at a time) or begin with all potential variables in the model and proceed backward (i.e. remove one variable at a time). At each step, and for each variable currently in the model, the program calculates the t-statistic for its estimated coefficient. For each variable not in the model, it computes the t-statistic that its coefficient would have if it were the next variable added, and squares it. At the next step, the program automatically enters the variable with the highest statistic (forward), or removes the variable with the lowest statistic (backward). In general, as in this case, if you have a modest-sized set of potential variables from which you wish to eliminate a few (i.e., if you're fine-tuning some prior selection of variables), you should generally go backward.

Stepwise Logistic Regression with R Akaike information criterion (AIC), where AIC = 2k - 2 log L = 2k + Deviance, where k = number of parameters. In general, smaller numbers are better.  Stepwise Logistic Regression penalizes models with many independent or predictor parameters or models with poor fit.  In general, the lower value of AIC suggests a "better" model, but it is a relative measure of model fit. It is used for model selection (i.e. it lets you compare different models estimated on the same dataset. Backwards selection is the default in the Logistic Regression method. Although there may be some evidence in the logistic regression literature that backward selection is less successful than forward selection. This may be due to the fact that the full model fit in the first step is the model most likely to result in a complete or quasi-complete separation of response values. However, backward seemed to be successful in this case. As a warning, since the interpretation of coefficients in a model depends on the other terms included, it may seem unwise to let an automatic algorithm determine the questions that we should ask about our data. The decision which variables to include into an analysis should be based on theory. However, there is little theory about these variables, so we need to operate on common business application.

### 2.1 Stepwise Regression using kNN Imputed Data

```{r}
churn_model_knn <- glm(churn~.,data=cdf_knn.imputed,family="binomial")
summary(churn_model_knn)

```

```{r}
churn_stepwise_knn = step(churn_model_knn, direction = c("backward"))

```

```{r}

```


### 2.2 Stepwise Regression using Randomforest Imputed Data

```{r}
churn_model_rf <- glm(churn~.,data=cdf_rf.imputed,family="binomial")
summary(churn_model_rf)
```

```{r}
churn_stepwise_rf = step(churn_model_rf, direction = c("backward"))
```

```{r}

```



```{r}
#xtabs(~ international_plan + churn, data = cdf_rf.imputed)
#xtabs(~ state + churn, data = cdf_rf.imputed) %>% t()
#xtabs(~ state + churn, data = cdf_rf.imputed) %>% chisq.test(correct = T)
```


```{r eval = F}
# Split data into training (70%) and validation (30%)

# Creates a value for dividing the data into train and test. In this case the value is defined as 75% of the number of rows in the dataset
smp_size = cdf_rf.imputed %>%
  nrow() %>%
  multiply_by(0.70) %>%
  floor()  

# Randomly identifies the rows equal to sample size from all the rows of cdf_rf.imputed dataset and stores the row number in train_ind
train_index <- cdf_rf.imputed %>%
  nrow() %>%
  sample(x = seq_len(.), size = smp_size)

train_df <- cdf_rf.imputed[train_index,]
test_df <- cdf_rf.imputed[-train_index,]

nrow(train_df)
nrow(test_df)
```


```{r}
#Run Logistic Regression
model4 <- glm(churn~.,data=cdf_rf.imputed,family="binomial")
summary(model4)
```

```{r}
#Stepwise Logistic Regression
mylogit_df = step(model4, direction = c("backward"))
```




As evident from the final stage of the stepwise regression, the final model in terms of selection was churn ~ international_plan + voice_mail_plan + total_day_minutes +
    total_day_charge + total_eve_charge + total_night_minutes +
    total_intl_charge + number_customer_service_calls

In general, the best model is the one with the lowest AIC possible in the logistic regression model with churn as the dependent variable. The final model with the most important variables in predicting churn were, in ascending order: total_day_minutes, total_night_minutes, total_intl_charge, total_eve_charge, voice_mail_,plan,  total_day_charge,  number_customer_service_calls , and international_plan. In other words, international_plan was the best predictor of churn, followed by number_customer_service_calls, etc. In backward, starting out with the full model, the single best predictor was and international_plan.  IThis procedure was used to help in the creation of a best predicted model for churn as the dependent variable.


```{r}
#Run Logistic Regression
model4 <- glm(churn~.,data=cdf_rf.imputed,family="binomial")
summary(model4)
summary(model4)$coefficient
#Stepwise Logistic Regression
mylogit_df = step(model4)
#Logistic Regression Coefficient
summary.coeff0 = summary(mylogit_df)$coefficient
#Calculating Odd Ratios
OddRatio = exp(coef(mylogit_df))
summary.coeff = cbind(Variable = row.names(summary.coeff0), OddRatio, summary.coeff0)
row.names(summary.coeff) = NULL
#
#R Function : Standardized Coefficients
stdz.coff <- function (regmodel)
{ b <- summary(regmodel)$coef[-1,1]
  sx <- sapply(regmodel$model[-1], sd)
  beta <-(3^(1/2))/pi * sx * b
  return(beta)
}

std.Coeff = data.frame(Standardized.Coeff = stdz.coff(mylogit_df))
std.Coeff = cbind(Variable = row.names(std.Coeff), std.Coeff)
row.names(std.Coeff) = NULL
#
#Final Summary Report
final = merge(summary.coeff, std.Coeff, by = "Variable", all.x = TRUE)
#
#Prediction
pred = predict(mylogit_df,val, type = "response")
finaldata = cbind(val, pred)
#
#Storing Model Performance Scores
library(ROCR)
pred_val <-prediction(pred ,finaldata$churn)
#
# Maximum Accuracy and prob. cutoff against it
acc.perf <- performance(pred_val, "acc")
ind = which.max( slot(acc.perf, "y.values")[[1]])
acc = slot(acc.perf, "y.values")[[1]][ind]
cutoff = slot(acc.perf, "x.values")[[1]][ind]
#
# Print Results
print(c(accuracy= acc, cutoff = cutoff))
#
# Calculating Area under Curve
perf_val <- performance(pred_val,"auc")
perf_val
#
# Plotting Lift curve
plot(performance(pred_val, measure="lift", x.measure="rpp"), colorize=TRUE)
#
# Plot the ROC curve
perf_val2 <- performance(pred_val, "tpr", "fpr")
plot(perf_val2, col = "green", lwd = 1.5)
#
#Calculating KS statistics
ks1.tree <- max(attr(perf_val2, "y.values")[[1]] - (attr(perf_val2, "x.values")[[1]]))
ks1.tree

```

```{r}
mylogit_df3 = step(model4, direction = c("both"))
```

#### Standardization of area_code, international_plan, voice_mail_plan for KNN imputation
```{r}
area_fn <- function(area_code) {
  if(area_code == "area_code_408"){
    return(0)
  } else if (area_code == "area_code_415") {
    return(1)
  } else {
    return(2)
  }
}

x <- churn_df_1 %>%
  transform(
    churn = ifelse(churn == "no", 0, 1),
    international_plan = ifelse(international_plan == "no", 0, 1),
    voice_mail_plan = ifelse(voice_mail_plan == "no", 0,1),
    area_code = sapply(area_code, area_fn)
  )
x
```
## checking for NA values in standardized dataset
```{r}
anyNA(x)
```

##checking for NA values in KNN imputed dataset

```{r}
knnOutput <- knnImputation(x)    #[, !names(churn_df_1) %in% "churn"])  # perform knn imputation.
anyNA(knnOutput)

```

### Building a model with KNN imputed dataset
```{r}
model5 <- glm(churn ~.,data=knnOutput,family="binomial")
summary(model5)
```
### Stepwise regression using the KNN imputed Model with direction = backward
```{r}
mylogit_df2 = step(model5, direction = c("backward"))
```

### Stepwise regression using the KNN imputed Model with direction = both
```{r}
mylogit_df3 = step(model5, direction = c("both"))
```

## Stepwise regression comparisons between randomforest and KNN imputed datasets
## Randomforest direction = backward
Step:  AIC=2062.31
churn ~ international_plan + voice_mail_plan + total_day_minutes +
    total_day_charge + total_eve_charge + total_night_minutes +
    total_intl_calls + total_intl_charge + number_customer_service_calls

## Randomforest direction = both
Step:  AIC=2062.31
churn ~ international_plan + voice_mail_plan + total_day_minutes +
    total_day_charge + total_eve_charge + total_night_minutes +
    total_intl_calls + total_intl_charge + number_customer_service_calls

## KNN direction = backward
Step:  AIC=2057.47
churn ~ international_plan + voice_mail_plan + total_day_minutes +
    total_day_charge + total_eve_minutes + total_night_minutes +
    total_intl_calls + total_intl_charge + number_customer_service_calls

## KNN direction = both
Step:  AIC=2057.47
churn ~ international_plan + voice_mail_plan + total_day_minutes +
    total_day_charge + total_eve_minutes + total_night_minutes +
    total_intl_calls + total_intl_charge + number_customer_service_calls

## The common variables in all the four stepwise regressions are :
international_plan
voice_mail_plan
total_day_minutes
total_day_charge
total_night_minutes
total_intl_calls
total_intl_charge
number_customer_service_calls
TOTAL = 8
## The additional variables not common in all four stepwise regressions are:
total_eve_minutes
total_eve_charge
TOTAL = 2
GRAND TOTAL = 10
## We suggest that these 10 variables be used in the next stage which is the model building process.





## Finding the best predictor variables

## Random forest significant attributes
international_planyes          2.1972515  0.1589164  13.826  < 2e-16 ***
voice_mail_planyes            -1.0275099  0.3676354  -2.795 0.005191 ** 
total_day_charge               0.0805890  0.0080508  10.010  < 2e-16 ***
total_eve_charge               0.0830480  0.0249265   3.332 0.000863 ***
total_intl_calls              -0.0837351  0.0267472  -3.131 0.001744 ** 
number_customer_service_calls  0.5395426  0.0420437  12.833  < 2e-16 ***
states (12) 
stateCA                        1.9772622  0.7938041   2.491 0.012743 *  
stateME                        1.2932594  0.7391097   1.750 0.080161 .  
stateMI                        1.4919750  0.7224169   2.065 0.038899 *  
stateMN                        1.2142000  0.7194116   1.688 0.091456 .  
stateMS                        1.2761173  0.7383695   1.728 0.083936 .  
stateMT                        1.7768485  0.7233884   2.456 0.014038 *  
stateNJ                        1.5639891  0.7174847   2.180 0.029271 *  
stateNV                        1.3044053  0.7301848   1.786 0.074034 .  
stateSC                        1.7432299  0.7519276   2.318 0.020430 *  
stateTX                        1.6774593  0.7127652   2.353 0.018600 *  
stateUT                        1.2332761  0.7465346   1.652 0.098534 .  
stateWA                        1.5031936  0.7340795   2.048 0.040587 *  
Only Random forest had an additional state MN as compared to KNN
## KNN significant attributes
international_plan             2.205e+00  1.594e-01  13.835  < 2e-16 ***
voice_mail_plan               -1.252e+00  4.395e-01  -2.849  0.00438 ** 
total_day_minutes             -5.762e-03  2.240e-03  -2.572  0.01010 *  
total_day_charge               1.074e-01  1.375e-02   7.813 5.58e-15 ***
total_eve_minutes              1.106e-02  4.403e-03   2.511  0.01204 *  
total_intl_calls              -8.585e-02  2.679e-02  -3.205  0.00135 ** 
number_customer_service_calls  5.432e-01  4.221e-02  12.868  < 2e-16 ***
states(11)
stateCA                        1.956e+00  7.908e-01   2.473  0.01339 *  
stateME                        1.256e+00  7.389e-01   1.700  0.08904 .  
stateMI                        1.465e+00  7.217e-01   2.030  0.04234 *  
stateMS                        1.253e+00  7.375e-01   1.699  0.08926 .  
stateMT                        1.754e+00  7.235e-01   2.424  0.01535 *  
stateNJ                        1.539e+00  7.166e-01   2.148  0.03173 *  
stateNV                        1.217e+00  7.316e-01   1.663  0.09632 .  
stateSC                        1.735e+00  7.498e-01   2.314  0.02064 *  
stateTX                        1.674e+00  7.108e-01   2.355  0.01852 *  
stateUT                        1.229e+00  7.463e-01   1.646  0.09968 .  
stateWA                        1.471e+00  7.321e-01   2.009  0.04455 *  

## We recommend the following variables for the initial model building(* denotes stepwise regression results)
international_planyes          
voice_mail_planyes            
total_day_charge               
total_eve_charge                                                  
total_intl_calls              
number_customer_service_calls 
total_day_minutes 
total_eve_minutes
total_night_minutes*
total_intl_charge*

## We recommend the following states for the initial model building
stateCA                        
stateME                          
stateMI                        
stateMN                          
stateMS                          
stateMT                        
stateNJ                        
stateNV                        
stateSC                        
stateTX                        
stateUT                          
stateWA














