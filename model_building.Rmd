---
title: "Model Building"
output: html_notebook
---

```{r warning = F}
library("magrittr")
library("ggplot2")
library("ROCR")
library("pROC")
```

```{r echo = F}
load("./data/data_prep.rda")
```

```{r echo = F}
# A function to split data into training (70%) and validation (30%)

create_data_partition <- function(dataset, train_size = 0.70) {
  # Creates a value for dividing the data into train and test.
  smp_size = cdf_rf.imputed %>%
    nrow() %>%
    multiply_by(train_size) %>%
    floor()

  # Randomly identifies the rows equal to sample size from all the rows of dataset dataset
  # and stores the row number in train_ind
  return(dataset %>%
           nrow() %>%
           sample(x = seq_len(.), size = smp_size)
  )
}
```

# 1. Spliting Dataset into Training and Test

```{r}
set.seed(123)

rf_train_index <- create_data_partition(cdf_rf.imputed)
knn_train_index <- create_data_partition(cdf_knn.imputed)

train_df_knn <- cdf_knn.imputed[knn_train_index,]
test_df_knn <- cdf_knn.imputed[-knn_train_index,]
train_df_rf <- cdf_rf.imputed[rf_train_index,]
test_df_rf <- cdf_rf.imputed[-rf_train_index,]
```

# 2. Building The Model
```{r}
modelRF <- glm(
  churn~international_plan +
    voice_mail_plan +
    total_day_charge +
    total_day_calls +
    total_eve_charge +
    total_eve_minutes +
    total_night_minutes +
    total_intl_charge +
    total_intl_calls +
    number_customer_service_calls,
  data = train_df_rf ,
  family = "binomial"
)

summary(modelRF)
```


```{r}
modelKNN <- glm(
  churn~international_plan +
    voice_mail_plan +
    total_day_charge +
    total_day_calls +
    total_eve_charge +
    total_eve_minutes +
    total_night_minutes +
    total_intl_charge +
    total_intl_calls +
    number_customer_service_calls,
  data = train_df_knn,
  family = "binomial"
)

summary(modelKNN)
```

# 3. Predicting & Evaluating Accuracy

## 3.1 Evaluation Accuracy of `modelRF`

```{r}
pred_churn_rf <- predict(modelRF, newdata = test_df_rf, type = "response")
roc_out_rf <- roc(test_df_rf$churn, pred_churn_rf)

roc_out_rf
```

```{r}
plot(roc_out_rf, col = "red", xlab = "False positive", ylab = "True positive")
```

The ROC curve is an evaluation method we used to assess the efficacy of binary characteristic algorithm as well as choose the optimal threshold based on our tolerance for false negatives and desire for true positives. Here we have a curve that shows a good result based on its useless, as displayed on the graph the x axis shows the specificity and the y axis shows the sensitivity. The area under the curve is used as a singular measure for assessing the usefulness of a classifier. For a perfect classifier the area under the ROC curve would be 1:


## 3.2 Evaluation accuracy of `modelKNN`
```{r}
pred_churn_knn <- predict(modelKNN, newdata = test_df_knn, type = "response")
roc_out_knn <- roc(test_df_knn$churn, pred_churn_knn)
roc_out_knn
```

```{r}
plot(roc_out_knn, col = "red", xlab = "False Positives", ylab = "True Positives")
```

# 4. Evaluation The Winining Model

```{r}
predicted_churn_status <- as.factor(pred_churn_rf > 0.2)
levels(predicted_churn_status)  <- list(no = "FALSE", yes = "TRUE")
confusion_matrix <- table(predicted_churn_status, actual_churn_status = test_df_rf$churn)

confusion_matrix
```

```{r}
save(modelKNN, modelRF, file = "data/model_building.rda")
```
